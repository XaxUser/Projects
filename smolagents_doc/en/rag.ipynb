{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XaxUser/Projects/blob/main/smolagents_doc/en/rag.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/huggingface/smolagents.git"
      ],
      "metadata": {
        "id": "qKxiT8Om0lHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install smolagents pandas langchain langchain-community sentence-transformers datasets python-dotenv rank_bm25 --upgrade"
      ],
      "metadata": {
        "id": "q_cgz28R0weI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain langchain_google_genai"
      ],
      "metadata": {
        "id": "HHuNwZpE0zGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDNXfqW40eOz"
      },
      "source": [
        "## Building an Agentic RAG System"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from smolagents import Tool\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "import datasets\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from dotenv import load_dotenv"
      ],
      "metadata": {
        "id": "NL7hiu6e0vZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "os.environ[\"HF_TOKEN\"] = hf_token"
      ],
      "metadata": {
        "id": "Pk2m3kOc04NH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "genai.configure(api_key=api_key)"
      ],
      "metadata": {
        "id": "fYtUyhx004QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()"
      ],
      "metadata": {
        "id": "VQmgBWoG04Su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Hugging Face documentation dataset\n",
        "knowledge_base = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n",
        "\n",
        "# Filter to include only Transformers documentation\n",
        "knowledge_base = knowledge_base.filter(lambda row: row[\"source\"].startswith(\"huggingface/transformers\"))\n",
        "\n",
        "# Convert dataset entries to Document objects with metadata\n",
        "source_docs = [\n",
        "    Document(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"].split(\"/\")[1]})\n",
        "    for doc in knowledge_base\n",
        "]\n",
        "\n",
        "# Split documents into smaller chunks for better retrieval\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,  # Characters per chunk\n",
        "    chunk_overlap=50,  # Overlap between chunks to maintain context\n",
        "    add_start_index=True,\n",
        "    strip_whitespace=True,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],  # Priority order for splitting\n",
        ")\n",
        "docs_processed = text_splitter.split_documents(source_docs)\n",
        "\n",
        "print(f\"Knowledge base prepared with {len(docs_processed)} document chunks\")"
      ],
      "metadata": {
        "id": "KEAweKg304Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RetrieverTool(Tool):\n",
        "    name = \"retriever\"\n",
        "    description = \"Uses semantic search to retrieve the parts of transformers documentation that could be most relevant to answer your query.\"\n",
        "    inputs = {\n",
        "        \"query\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"The query to perform. This should be semantically close to your target documents. Use the affirmative form rather than a question.\",\n",
        "        }\n",
        "    }\n",
        "    output_type = \"string\"\n",
        "\n",
        "    def __init__(self, docs, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Initialize the retriever with our processed documents\n",
        "        self.retriever = BM25Retriever.from_documents(\n",
        "            docs, k=10  # Return top 10 most relevant documents\n",
        "        )\n",
        "\n",
        "    def forward(self, query: str) -> str:\n",
        "        \"\"\"Execute the retrieval based on the provided query.\"\"\"\n",
        "        assert isinstance(query, str), \"Your search query must be a string\"\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        docs = self.retriever.invoke(query)\n",
        "\n",
        "        # Format the retrieved documents for readability\n",
        "        return \"\\nRetrieved documents:\\n\" + \"\".join(\n",
        "            [\n",
        "                f\"\\n\\n===== Document {str(i)} =====\\n\" + doc.page_content\n",
        "                for i, doc in enumerate(docs)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "# Initialize our retriever tool with the processed documents\n",
        "retriever_tool = RetrieverTool(docs_processed)"
      ],
      "metadata": {
        "id": "O5KSpsiO04Ye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the ChatGoogleGenerativeAI model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",  # You can specify the model ID here\n",
        "    verbose=True  # Enable verbose to display detailed output\n",
        ")\n",
        "\n",
        "# Define a prompt template for your question\n",
        "prompt_template = \"Answer the following query based on the retrieved documents: {query}\"\n",
        "prompt = PromptTemplate(input_variables=[\"query\"], template=prompt_template)\n",
        "\n",
        "# Set up an LLM chain to connect the model with the retriever\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "QTFEqzZa04at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to use retriever and then pass the result to ChatGoogleGenerativeAI for final response\n",
        "def ask_question_with_retriever(question: str):\n",
        "    # Step 1: Retrieve relevant documents using the retriever tool\n",
        "    print(\"Step 1: Retrieving relevant documents based on the query...\")\n",
        "    retrieved_docs = retriever_tool.forward(question)\n",
        "    print(retrieved_docs)  # Show the retrieved documents\n",
        "\n",
        "    # Step 2: Pass the retrieved documents to the LLM chain for final response\n",
        "    print(\"Step 2: Generating the final answer using the model...\")\n",
        "    response = llm_chain.run({\"query\": question + \"\\n\" + retrieved_docs})\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "ZKVCAcnk1XaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a question that requires retrieving information\n",
        "question = \"For a transformers model training, which is slower, the forward or the backward pass?\"\n",
        "\n",
        "# Get the answer from the agent\n",
        "agent_output = ask_question_with_retriever(question)\n",
        "\n",
        "# Display the final answer\n",
        "print(\"\\nFinal answer:\")\n",
        "print(agent_output)"
      ],
      "metadata": {
        "id": "Skr77fwt1auG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}